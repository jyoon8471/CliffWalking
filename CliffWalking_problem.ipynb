{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sarsa and Q-learning for Cliff Walking\n",
    "\n",
    "## Game-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Cliff walking game\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from python_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Game setup\n",
    "WORLD_HEIGHT = 4\n",
    "WORLD_WIDTH = 12\n",
    "\n",
    "ACTION_UP = 0\n",
    "ACTION_DOWN = 1\n",
    "ACTION_LEFT = 2\n",
    "ACTION_RIGHT = 3\n",
    "actions = [ACTION_UP, ACTION_DOWN, ACTION_LEFT, ACTION_RIGHT]\n",
    "\n",
    "startState = [3, 0]\n",
    "goalState = [3, 11]\n",
    "\n",
    "\n",
    "# Initial Q-function\n",
    "stateActionValues = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
    "\n",
    "\n",
    "# state-action scores\n",
    "actionRewards = np.zeros((WORLD_HEIGHT, WORLD_WIDTH, 4))\n",
    "actionRewards[:, :, :] = -1\n",
    "actionRewards[2, 1:11, ACTION_DOWN] = -100.0  # Falling into the cliff\n",
    "actionRewards[3, 0, ACTION_RIGHT] = -100.0\n",
    "\n",
    "\n",
    "# Game algorithm giving next_state & reward & termination\n",
    "def step(currentState, action):\n",
    "    current_x = currentState[0]\n",
    "    current_y = currentState[1]\n",
    "    \n",
    "    nextState = []\n",
    "    \n",
    "    if (action == ACTION_UP):\n",
    "        next_x = max(0, current_x - 1)\n",
    "        next_y = current_y\n",
    "    elif (action == ACTION_DOWN):\n",
    "        next_x = min(3, current_x + 1)\n",
    "        next_y = current_y\n",
    "    elif (action == ACTION_LEFT):\n",
    "        next_x = current_x\n",
    "        next_y = max(0, current_y - 1)\n",
    "    else:\n",
    "        next_x = current_x\n",
    "        next_y = min(11, current_y + 1)\n",
    "    \n",
    "    if (currentState == startState and action == ACTION_RIGHT): ## Falling into the cliff at the start state\n",
    "        next_x = startState[0]\n",
    "        next_y = startState[1]\n",
    "        \n",
    "    if (next_x == 3 and 1<= next_y <= 10):\n",
    "        nextState = startState\n",
    "    else:\n",
    "        nextState = [next_x, next_y]\n",
    "        \n",
    "    reward = actionRewards[currentState[0], currentState[1], action]\n",
    "    \n",
    "    if (nextState == goalState):\n",
    "        terminal = True\n",
    "    else:\n",
    "        terminal = False\n",
    "        \n",
    "    return nextState, reward, terminal\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function of $\\epsilon$-greedy action selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chooseAction(state, stateActionValues, epsilon):\n",
    "    if np.random.binomial(1, epsilon) == 1:    # Exploration with probability epsilon\n",
    "        return np.random.choice(actions)\n",
    "    else:                                      # Greedy action selection\n",
    "        return np.argmax(stateActionValues[state[0], state[1], :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables : Hyper Parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPSILON = 0.1   # epsilon-greedy action selection\n",
    "ALPHA = 0.5     # Learning rate\n",
    "GAMMA = 1.0     # Discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running with SARSA algorithm\n",
    "\n",
    "#### - Please fill out the missing parts of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sarsa(stateActionValues, stepsize = ALPHA):\n",
    "    \n",
    "    currentState = startState\n",
    "    currentAction = chooseAction(currentState, stateActionValues, EPSILON)\n",
    "    accumulatedRewards = 0.0\n",
    "    terminal = False\n",
    "    \n",
    "    while terminal != True:\n",
    "        \n",
    "        # 1. Step the current action a & Observe the next_state, reward, terminal\n",
    "        newState, reward, terminal = step(currentState, currentAction)\n",
    "        \n",
    "        # 2. Choose new action by ON-POLICY on the next_state\n",
    "        newAction = chooseAction(newState, stateActionValues, EPSILON)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################### Please write down the update rule of Q-value #########################\n",
    "        \n",
    "        # 3. Incremental update of Q-function\n",
    "        \n",
    "        # New action selection for New state with ON-POLICY \n",
    "        newStateActionValue = ???\n",
    "        \n",
    "        \n",
    "        # Q-function Update\n",
    "        stateActionValues[currentState[0], currentState[1], currentAction] = ???\n",
    "        \n",
    "        \n",
    "        ###############################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 4. Update the timestep\n",
    "        currentState = newState\n",
    "        currentAction = newAction\n",
    "        \n",
    "        # 5. Accumulation of reward\n",
    "        accumulatedRewards += reward\n",
    "        \n",
    "    return accumulatedRewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running with Q-learning algorithm\n",
    "\n",
    "#### - Please fill out the missing parts of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def qLearning(stateActionValues, stepsize = ALPHA):\n",
    "    \n",
    "    currentState = startState\n",
    "    accumulatedRewards = 0.0\n",
    "    terminal = False\n",
    "    \n",
    "    while terminal != True:\n",
    "        \n",
    "        # 1. Choose the action from Q-function\n",
    "        currentAction = chooseAction(currentState, stateActionValues, EPSILON)\n",
    "        \n",
    "        # 2. Step the current action a & Observe the next_state, reward, terminal\n",
    "        newState,reward, terminal = step(currentState, currentAction)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ######################### Please write down the update rule of Q-value #########################\n",
    "        \n",
    "        # 3. Incremental update of Q-function\n",
    "        \n",
    "        # New action selection for New state with OFF-POLICY \n",
    "        newStateActionValue = ???\n",
    "        \n",
    "        \n",
    "        # Q-function Update\n",
    "        stateActionValues[currentState[0], currentState[1], currentAction] = ???\n",
    "        \n",
    "        \n",
    "        ###############################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # 5. Update the timestep\n",
    "        currentState = newState\n",
    "        \n",
    "        # 6. Accumulation of reward\n",
    "        accumulatedRewards += reward\n",
    "        \n",
    "    return accumulatedRewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the convergence of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# episodes of each run\n",
    "nEpisodes = 500\n",
    "\n",
    "# perform 20 independent runs\n",
    "runs = 20\n",
    "\n",
    "rewardsSarsa = np.zeros(nEpisodes)\n",
    "rewardsQLearning = np.zeros(nEpisodes)\n",
    "for run in range(0, runs):\n",
    "    stateActionValuesSarsa = np.copy(stateActionValues)\n",
    "    stateActionValuesQLearning = np.copy(stateActionValues)\n",
    "    for i in range(0, nEpisodes):\n",
    "        # cut off the value by -100 to draw the figure more elegantly\n",
    "        # whenever you call sarsa(Q), or qLearning(Q), \n",
    "        # it will update Q and give you the accumulated reward resulted from the current ep.\n",
    "        rewardsSarsa[i] += max(sarsa(stateActionValuesSarsa), -100)\n",
    "        rewardsQLearning[i] += max(qLearning(stateActionValuesQLearning), -100)\n",
    "\n",
    "# averaging over independt runs\n",
    "rewardsSarsa /= runs\n",
    "rewardsQLearning /= runs\n",
    "\n",
    "# Smoothing the episodic rewards\n",
    "averageRange = 10\n",
    "smoothedRewardsSarsa = np.copy(rewardsSarsa)\n",
    "smoothedRewardsQLearning = np.copy(rewardsQLearning)\n",
    "for i in range(averageRange, nEpisodes):\n",
    "    smoothedRewardsSarsa[i] = np.mean(rewardsSarsa[i - averageRange: i + 1])\n",
    "    smoothedRewardsQLearning[i] = np.mean(rewardsQLearning[i - averageRange: i + 1])\n",
    "\n",
    "# draw reward curves\n",
    "plt.figure(1)\n",
    "plt.plot(smoothedRewardsSarsa, label='Sarsa')\n",
    "plt.plot(smoothedRewardsQLearning, label='Q-Learning')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Sum of rewards during episode')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Optimal Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print optimal policy\n",
    "def printOptimalPolicy(stateActionValues):\n",
    "    optimalPolicy = []\n",
    "    for i in range(0, WORLD_HEIGHT):\n",
    "        optimalPolicy.append([])\n",
    "        for j in range(0, WORLD_WIDTH):\n",
    "            if [i, j] == goalState:\n",
    "                optimalPolicy[-1].append('G')\n",
    "                continue\n",
    "            bestAction = np.argmax(stateActionValues[i, j, :])\n",
    "            if bestAction == ACTION_UP:\n",
    "                optimalPolicy[-1].append('U')\n",
    "            elif bestAction == ACTION_DOWN:\n",
    "                optimalPolicy[-1].append('D')\n",
    "            elif bestAction == ACTION_LEFT:\n",
    "                optimalPolicy[-1].append('L')\n",
    "            elif bestAction == ACTION_RIGHT:\n",
    "                optimalPolicy[-1].append('R')\n",
    "    for row in optimalPolicy:\n",
    "        print (row)\n",
    "\n",
    "# display optimal policy\n",
    "print ('Sarsa Optimal Policy:')\n",
    "printOptimalPolicy(stateActionValuesSarsa)\n",
    "print ('Q-Learning Optimal Policy:')\n",
    "printOptimalPolicy(stateActionValuesQLearning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
